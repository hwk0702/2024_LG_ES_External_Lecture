{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "SHF_eN9rPJiT",
   "metadata": {
    "id": "SHF_eN9rPJiT"
   },
   "source": [
    "# [ LG에너지솔루션 DX Expert 양성과정 ]\n",
    "\n",
    "Vision Transformer를 활용한 image classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2705836-e2fe-47ca-a90d-381f74d55d45",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/hwk0702/2024_LG_ES_External_Lecture/blob/main/240703_Transformer/image/ViT01.png?raw=true\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a928594-2edd-4df5-bf68-d713bb7497c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dbc71e",
   "metadata": {
    "id": "65dbc71e"
   },
   "outputs": [],
   "source": [
    "!pip install timm==0.9.2\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd6783d",
   "metadata": {
    "id": "bcd6783d"
   },
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3e500",
   "metadata": {
    "id": "9ed3e500"
   },
   "outputs": [],
   "source": [
    "# 필수 라이브러리 임포트\n",
    "import os  # 운영체제 관련 기능\n",
    "import random  # 무작위 수 생성\n",
    "import numpy as np  # 숫자 계산\n",
    "import pandas as pd  # 데이터 처리 및 분석\n",
    "from collections import defaultdict  # 기본값이 있는 딕셔너리 생성\n",
    "from time import time  # 시간 측정\n",
    "from tqdm.auto import tqdm  # 진행률 표시\n",
    "\n",
    "# scikit-learn의 평가 지표 임포트\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# PyTorch 라이브러리 임포트\n",
    "import torch  # 딥러닝 프레임워크\n",
    "import torch.nn as nn  # 신경망 모듈\n",
    "from torch.optim import SGD  # 최적화 알고리즘\n",
    "from torch.utils.data import Dataset, DataLoader  # 데이터셋 및 데이터 로더\n",
    "from torchvision import transforms, datasets  # 데이터 변환 및 데이터셋\n",
    "\n",
    "# TIMM 라이브러리 임포트 (모델 생성용)\n",
    "from timm import list_models, create_model\n",
    "\n",
    "# OpenCV와 Matplotlib 임포트 (이미지 처리 및 시각화)\n",
    "import cv2  # 이미지 처리 라이브러리\n",
    "import matplotlib.pyplot as plt  # 시각화 라이브러리\n",
    "\n",
    "# Seaborn 설정 (더 나은 시각화 설정)\n",
    "import seaborn as sns\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
    "\n",
    "# 추가 라이브러리 임포트\n",
    "from einops import rearrange  # 텐서 조작에 유용, 필요한 경우 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a71ddf",
   "metadata": {
    "id": "00a71ddf"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d16ab",
   "metadata": {
    "id": "784d16ab"
   },
   "outputs": [],
   "source": [
    "def torch_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "    # CUDA randomness\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "\n",
    "def calc_metrics(y_true: list, y_score: np.ndarray, y_pred: list) -> dict:\n",
    "    # softmax\n",
    "    y_score = nn.functional.softmax(torch.FloatTensor(y_score), dim=1)\n",
    "\n",
    "    # metrics\n",
    "    auroc = roc_auc_score(y_true, y_score, average='macro', multi_class='ovr')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    return {\n",
    "        'auroc'    : auroc,\n",
    "        'f1'       : f1,\n",
    "        'recall'   : recall,\n",
    "        'precision': precision\n",
    "    }\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, log_interval: int, device: str) -> list:\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_score = []\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        # convert device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # model outputs\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update model weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # total loss and acc\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        correct += targets.eq(preds).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        total_score.extend(outputs.cpu().tolist())\n",
    "        total_preds.extend(preds.cpu().tolist())\n",
    "        total_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "        # log learning history\n",
    "        if i % log_interval == 0 or (i+1) == len(dataloader):\n",
    "            print('TRAIN [%5d/%5d]: Loss: %.3f | Acc: %.3f%% [%5d/%5d]' %\n",
    "                 (i+1, len(dataloader), total_loss/(i+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # calculate metrics\n",
    "    metrics = calc_metrics(\n",
    "        y_true  = total_targets,\n",
    "        y_score = total_score,\n",
    "        y_pred  = total_preds\n",
    "    )\n",
    "\n",
    "    metrics.update([('acc',correct/total), ('loss',total_loss/len(dataloader))])\n",
    "\n",
    "    # logging metrics\n",
    "    print('\\nTRAIN: Loss: %.3f | Acc: %.3f%% | AUROC: %.3f%% | F1-Score: %.3f%% | Recall: %.3f%% | Precision: %.3f%%\\n' %\n",
    "         (metrics['loss'], 100.*metrics['acc'], 100.*metrics['auroc'], 100.*metrics['f1'], 100.*metrics['recall'], 100.*metrics['precision']))\n",
    "\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def test(model, dataloader, criterion, log_interval: int, device: str) -> list:\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_score = []\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "\n",
    "    torch_seed(223)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            # convert device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # model outputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # total loss and acc\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            correct += targets.eq(preds).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "            total_score.extend(outputs.cpu().tolist())\n",
    "            total_preds.extend(preds.cpu().tolist())\n",
    "            total_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "            # log learning history\n",
    "            if i % log_interval == 0 or (i+1) == len(dataloader):\n",
    "                print('TEST [%5d/%5d]: Loss: %.3f | Acc: %.3f%% [%5d/%5d]' %\n",
    "                      (i+1, len(dataloader), total_loss/(i+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # calculate metrics\n",
    "    metrics = calc_metrics(\n",
    "        y_true  = total_targets,\n",
    "        y_score = total_score,\n",
    "        y_pred  = total_preds\n",
    "    )\n",
    "\n",
    "    metrics.update([('acc',correct/total), ('loss',total_loss/len(dataloader))])\n",
    "\n",
    "    # logging metrics\n",
    "    print('\\nTEST: Loss: %.3f | Acc: %.3f%% | AUROC: %.3f%% | F1-Score: %.3f%% | Recall: %.3f%% | Precision: %.3f%%\\n' %\n",
    "         (metrics['loss'], 100.*metrics['acc'], 100.*metrics['auroc'], 100.*metrics['f1'], 100.*metrics['recall'], 100.*metrics['precision']))\n",
    "\n",
    "    # return\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def fit(\n",
    "    model, trainloader, testloader, criterion, optimizer, epochs: int, log_interval: int, device: str) -> list:\n",
    "\n",
    "    train_history = defaultdict(list)\n",
    "    test_history = defaultdict(list)\n",
    "\n",
    "    # fitting model\n",
    "    for i in range(epochs):\n",
    "        print(f'\\nEpoch: [{i+1}/{epochs}]')\n",
    "        train_metrics = train(\n",
    "            model        = model,\n",
    "            dataloader   = trainloader,\n",
    "            criterion    = criterion,\n",
    "            optimizer    = optimizer,\n",
    "            log_interval = log_interval,\n",
    "            device       = device,\n",
    "        )\n",
    "\n",
    "        test_metrics = test(\n",
    "            model        = model,\n",
    "            dataloader   = testloader,\n",
    "            criterion    = criterion,\n",
    "            log_interval = log_interval,\n",
    "            device       = device\n",
    "        )\n",
    "\n",
    "        # stack history\n",
    "        for k, v in train_metrics.items():\n",
    "            train_history[k].append(v)\n",
    "        for k, v in test_metrics.items():\n",
    "            test_history[k].append(v)\n",
    "\n",
    "    return train_history, test_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b247e0",
   "metadata": {
    "id": "33b247e0"
   },
   "source": [
    "# Configuration for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f8b66",
   "metadata": {
    "id": "398f8b66"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # dataset 관련 parameters\n",
    "    datadir = './data'\n",
    "    image_size = [32, 32]\n",
    "    num_classes = 10\n",
    "\n",
    "    # training 관련 parameters\n",
    "    epochs = 10\n",
    "    batch_size = 32\n",
    "    test_batch_size = 128\n",
    "    learning_rate = 0.001\n",
    "    num_workers = 2\n",
    "    log_interval = 200\n",
    "\n",
    "    # device\n",
    "    device = 'cuda'\n",
    "\n",
    "    # seed\n",
    "    seed = 223\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177c24d",
   "metadata": {
    "id": "a177c24d"
   },
   "source": [
    "# Load dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d42e4",
   "metadata": {
    "id": "672d42e4",
    "outputId": "0f9b5a00-082e-4837-a76e-9c49ea06b382"
   },
   "outputs": [],
   "source": [
    "trainset = datasets.CIFAR10(root=cfg.datadir, train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.CIFAR10(root=cfg.datadir, train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718dc41",
   "metadata": {
    "id": "6718dc41"
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "testloader = DataLoader(testset, batch_size=cfg.test_batch_size, shuffle=False, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20aacc6",
   "metadata": {
    "id": "c20aacc6",
    "outputId": "83360b05-f9c2-4f8f-deb9-4b2f4f767e55"
   },
   "outputs": [],
   "source": [
    "print('the number of images in trainset: ',len(trainset.data))\n",
    "print('the number of images in testset: ',len(testset.data))\n",
    "print()\n",
    "print('image size: ',trainset.data[0].shape)\n",
    "print('target category')\n",
    "print(trainset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd043f3e",
   "metadata": {
    "id": "bd043f3e",
    "outputId": "508deb00-07bf-49e0-e58b-3c65f2a79e39"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(20,8))\n",
    "\n",
    "for i in range(10):\n",
    "    idx = torch.where(torch.Tensor(trainset.targets)==i)[0][0]\n",
    "    ax[i//5, i%5].imshow(trainset[idx][0].permute(1,2,0))\n",
    "    ax[i//5, i%5].set_title(list(trainset.class_to_idx.keys())[i], size=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7088e14",
   "metadata": {
    "id": "d7088e14"
   },
   "source": [
    "# Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb8e78",
   "metadata": {
    "id": "4fdb8e78"
   },
   "source": [
    "## without pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a83cd",
   "metadata": {
    "id": "7c6a83cd",
    "outputId": "afbdc40b-391f-407b-e08f-a7c8056c21f2"
   },
   "outputs": [],
   "source": [
    "model_wo_pretrained = create_model(\n",
    "    model_name  = 'vit_base_patch8_224',\n",
    "    img_size    = cfg.image_size,\n",
    "    num_classes = cfg.num_classes\n",
    ")\n",
    "model_wo_pretrained.to(cfg.device)\n",
    "print('load model')\n",
    "print('The number of model parameters: ',sum([p.numel() for p in model_wo_pretrained.parameters()]))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model_wo_pretrained.parameters(), lr=cfg.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35541108",
   "metadata": {
    "id": "35541108",
    "outputId": "83e07b5d-2810-4306-8b96-0fd7c11675e7"
   },
   "outputs": [],
   "source": [
    "img_temp = trainset[0][0]\n",
    "plt.imshow(img_temp.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882adf5",
   "metadata": {
    "id": "b882adf5",
    "outputId": "eddc25f9-e05c-44d4-fba8-d31be5b194d8"
   },
   "outputs": [],
   "source": [
    "# patchify\n",
    "img_temp_patch = rearrange(img_temp, 'c (h ph) (w pw) -> (h w) (ph pw c)', ph=8, pw=8)\n",
    "\n",
    "fig, ax = plt.subplots(4,4,figsize=(7,7))\n",
    "\n",
    "for i in range(len(img_temp_patch)):\n",
    "    ax[i//4, i%4].imshow(img_temp_patch[i].reshape(8, 8, 3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fcf425",
   "metadata": {
    "id": "52fcf425",
    "outputId": "d31d3368-5f61-43e9-c3c7-50f8f710ecb0"
   },
   "outputs": [],
   "source": [
    "# patch embedding\n",
    "temp_out = model_wo_pretrained.patch_embed(img_temp.unsqueeze(0).to(cfg.device))\n",
    "print(temp_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f3695",
   "metadata": {
    "id": "7e7f3695",
    "outputId": "c7b09f59-8806-4219-f65b-e3c8e19a4a7f"
   },
   "outputs": [],
   "source": [
    "# encoder outputs\n",
    "temp_out = model_wo_pretrained.forward_features(img_temp.unsqueeze(0).to(cfg.device))\n",
    "print(temp_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166742f",
   "metadata": {
    "id": "b166742f",
    "outputId": "be3dabb3-6e9d-4527-ce79-bd5c08aecf3b"
   },
   "outputs": [],
   "source": [
    "# head outputs\n",
    "temp_out = model_wo_pretrained.head(temp_out[:,0,:])\n",
    "print(temp_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac7c25",
   "metadata": {
    "id": "5bac7c25"
   },
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149f780",
   "metadata": {
    "id": "e149f780",
    "outputId": "26544bd5-533e-492b-a876-02a5d1a09cb2"
   },
   "outputs": [],
   "source": [
    "torch_seed(cfg.seed)\n",
    "train_history_wo_pretrained, test_history_wo_pretrained = fit(\n",
    "    model        = model_wo_pretrained,\n",
    "    trainloader  = trainloader,\n",
    "    testloader   = testloader,\n",
    "    criterion    = criterion,\n",
    "    optimizer    = optimizer,\n",
    "    epochs       = cfg.epochs,\n",
    "    log_interval = cfg.log_interval,\n",
    "    device       = cfg.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a11a6b",
   "metadata": {
    "id": "87a11a6b"
   },
   "outputs": [],
   "source": [
    "def figure(all_train_history: list, all_test_history: list, all_exp_name: list) -> None:\n",
    "    fig, ax = plt.subplots(2, 6, figsize=(20,7))\n",
    "\n",
    "    # train line plot\n",
    "    for train_h, exp_name in zip(all_train_history, all_exp_name):\n",
    "        for i, (k, v) in enumerate(train_h.items()):\n",
    "            sns.lineplot(\n",
    "                x     = range(1, len(v)+1),\n",
    "                y     = v,\n",
    "                label = exp_name,\n",
    "                ax    = ax[0, i]\n",
    "            )\n",
    "            # set y axis label\n",
    "            ax[0, i].set_ylabel(f'{k.upper()}')\n",
    "            # set x axis label\n",
    "            ax[0, i].set_xlabel('Epochs')\n",
    "            # set title\n",
    "            ax[0, i].set_title(f'{k.upper()} of Trainset')\n",
    "            # set legend\n",
    "            ax[0, i].legend(loc='lower right')\n",
    "            # set ylim\n",
    "            if k != 'loss':\n",
    "                ax[0, i].set_ylim(0, 1)\n",
    "\n",
    "    # test lineplot\n",
    "    for test_h, exp_name in zip(all_test_history, all_exp_name):\n",
    "        for i, (k, v) in enumerate(test_h.items()):\n",
    "            sns.lineplot(\n",
    "                x     = range(1, len(v)+1),\n",
    "                y     = v,\n",
    "                label = exp_name,\n",
    "                ax    = ax[1, i]\n",
    "            )\n",
    "            # set y axis label\n",
    "            ax[1, i].set_ylabel(f'{k.upper()}')\n",
    "            # set x axis label\n",
    "            ax[1, i].set_xlabel('Epochs')\n",
    "            # set title\n",
    "            ax[1, i].set_title(f'{k.upper()} of Testset')\n",
    "            # set legend\n",
    "            ax[1, i].legend(loc='lower right')\n",
    "            # set ylim\n",
    "            if k != 'loss':\n",
    "                ax[0, i].set_ylim(0, 1)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f525d4",
   "metadata": {
    "id": "28f525d4",
    "outputId": "c1525fdb-bd54-4f09-ea39-0a89b520901b"
   },
   "outputs": [],
   "source": [
    "all_train_history = [train_history_wo_pretrained]\n",
    "all_test_history = [test_history_wo_pretrained]\n",
    "all_exp_name = ['ViT-B/8 wo pretrained']\n",
    "\n",
    "figure(\n",
    "    all_train_history = all_train_history,\n",
    "    all_test_history  = all_test_history,\n",
    "    all_exp_name      = all_exp_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c3341",
   "metadata": {
    "id": "5a8c3341"
   },
   "source": [
    "## with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf4cdcb",
   "metadata": {
    "id": "ebf4cdcb",
    "outputId": "5b876eea-0f7b-47f8-de49-aa7a7800b3e0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_w_pretrained = create_model(\n",
    "    model_name  = 'vit_base_patch8_224.augreg_in21k',\n",
    "    img_size    = cfg.image_size,\n",
    "    num_classes = cfg.num_classes,\n",
    "    pretrained  = True\n",
    ")\n",
    "model_w_pretrained.to(cfg.device)\n",
    "print('load model')\n",
    "print('The number of model parameters: ',sum([p.numel() for p in model_w_pretrained.parameters()]))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model_w_pretrained.parameters(), lr=cfg.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf24fa6",
   "metadata": {
    "id": "fdf24fa6"
   },
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63bd53b",
   "metadata": {
    "id": "b63bd53b",
    "outputId": "a34e818d-0d8a-4b18-c0c6-5589edb6dfc3"
   },
   "outputs": [],
   "source": [
    "torch_seed(cfg.seed)\n",
    "train_history_w_pretrained, test_history_w_pretrained = fit(\n",
    "    model        = model_w_pretrained,\n",
    "    trainloader  = trainloader,\n",
    "    testloader   = testloader,\n",
    "    criterion    = criterion,\n",
    "    optimizer    = optimizer,\n",
    "    epochs       = cfg.epochs,\n",
    "    log_interval = cfg.log_interval,\n",
    "    device       = cfg.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d501c",
   "metadata": {
    "id": "f17d501c",
    "outputId": "61301a16-3125-4177-b429-85bf0a595b1a"
   },
   "outputs": [],
   "source": [
    "all_train_history.append(train_history_w_pretrained)\n",
    "all_test_history.append(test_history_w_pretrained)\n",
    "all_exp_name.append('ViT-B/8 w pretrained')\n",
    "\n",
    "figure(\n",
    "    all_train_history = all_train_history,\n",
    "    all_test_history  = all_test_history,\n",
    "    all_exp_name      = all_exp_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56332b6d",
   "metadata": {
    "id": "56332b6d"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c62c49",
   "metadata": {
    "id": "92c62c49"
   },
   "source": [
    "## Position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b11a2",
   "metadata": {
    "id": "977b11a2"
   },
   "outputs": [],
   "source": [
    "def pos_cos_figure(model):\n",
    "    # 모델의 위치 임베딩에서 첫 번째 패치의 길이를 가져옵니다.\n",
    "    num_patch = int(len(model.pos_embed[0]) - 1)\n",
    "    # 패치의 수에 따라 히트맵의 행과 열의 수를 계산합니다.\n",
    "    row = int(np.sqrt(num_patch))\n",
    "    col = int(np.sqrt(num_patch))\n",
    "\n",
    "    # 행과 열의 수를 바탕으로 서브플롯을 생성합니다.\n",
    "    fig, ax = plt.subplots(row, col, figsize=(7,7))\n",
    "\n",
    "    # 각 패치의 위치 임베딩 간의 유사도를 계산합니다.\n",
    "    for p_i in range(num_patch):\n",
    "        sim_matrix_i = np.zeros((row, col))\n",
    "        for p_j in range(num_patch):\n",
    "            sim_matrix_i[p_j//col, p_j%col] = nn.functional.cosine_similarity(\n",
    "                x1  = model.pos_embed[0, p_i+1, :],\n",
    "                x2  = model.pos_embed[0, p_j+1, :],\n",
    "                dim = 0\n",
    "            ).item()\n",
    "\n",
    "        # 계산된 유사도를 히트맵으로 시각화합니다.\n",
    "        sns.heatmap(sim_matrix_i, cmap='viridis', vmax=1, ax=ax[p_i//col, p_i%col])\n",
    "        ax[p_i//col, p_i%col].axis('off')\n",
    "\n",
    "    # 레이아웃을 조정하고, 히트맵을 출력합니다.\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9c1ae",
   "metadata": {
    "id": "fad9c1ae",
    "outputId": "8dfe825b-b7ad-45cb-d8e5-a3e870cf4b61"
   },
   "outputs": [],
   "source": [
    "pos_cos_figure(model_wo_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6c877",
   "metadata": {
    "id": "fca6c877",
    "outputId": "a88ab8b8-8663-432f-9fa9-5066b903e6ad"
   },
   "outputs": [],
   "source": [
    "pos_cos_figure(model_w_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e298f636",
   "metadata": {
    "id": "e298f636"
   },
   "source": [
    "## Visualize attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7763779",
   "metadata": {
    "id": "c7763779"
   },
   "outputs": [],
   "source": [
    "activation = {}\n",
    "\n",
    "# 어텐션 스코어를 계산하여 저장하는 함수\n",
    "def get_attn_softmax(name):\n",
    "    def hook(model, input, output):\n",
    "        with torch.no_grad():\n",
    "            input = input[0]\n",
    "            \n",
    "            # 입력의 크기를 가져옴\n",
    "            B, N, C = input.shape\n",
    "            \n",
    "            # QKV 분해\n",
    "            qkv = model.qkv(input).detach().reshape(B, N, 3, model.num_heads, C // model.num_heads) \n",
    "            # QKV의 shape: (B, N, 3, num_heads, C // num_heads)\n",
    "            qkv = qkv.permute(2, 0, 3, 1, 4) \n",
    "            # QKV의 shape: (3, B, num_heads, N, C // num_heads)\n",
    "            \n",
    "            # Q, K, V 추출\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "            # Q, K, V의 shape: (B, num_heads, N, C // num_heads)\n",
    "            \n",
    "            # 어텐션 계산\n",
    "            attn = (q @ k.transpose(-2, -1)) * model.scale\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            # attn의 shape: (B, num_heads, N, N)\n",
    "            \n",
    "            # 결과 저장\n",
    "            activation[name] = attn\n",
    "\n",
    "    return hook\n",
    "\n",
    "# 모델의 모든 블록에 어텐션 후크 추가\n",
    "def add_attn_vis_hook(model):\n",
    "    # 모델의 모든 블록을 순회합니다.\n",
    "    for idx, module in enumerate(model.blocks.children()):\n",
    "        # 각 블록의 어텐션 모듈에 get_attn_softmax 함수를 후크로 등록합니다.\n",
    "        module.attn.register_forward_hook(get_attn_softmax(f\"attn{idx}\"))\n",
    "\n",
    "# 어텐션 스코어와 이미지를 블렌딩하는 함수\n",
    "def blend_attention_with_image(image: np.ndarray, activation: dict):\n",
    "    # 어텐션 어그리게이션\n",
    "    attn_agg = torch.cat(list(activation.values())).sum(dim=(0,1))[0, 1:].softmax(dim=0).cpu().numpy()\n",
    "    # (num_layers * B, num_heads, N, N) -> (N, N) -> (N-1,)\n",
    "    attn_agg = attn_agg.reshape(4, 4)\n",
    "    attn_agg = cv2.resize(attn_agg, dsize=(32, 32))\n",
    "    \n",
    "    # 스케일링\n",
    "    attn_agg = (attn_agg - attn_agg.min()) / (attn_agg.max() - attn_agg.min())\n",
    "    \n",
    "    # 블렌딩\n",
    "    attn_agg = np.expand_dims(attn_agg, axis=-1) # (32, 32, 1)\n",
    "    img_blend = image * 0.5 + attn_agg * 0.8\n",
    "    \n",
    "    # 스케일링\n",
    "    img_blend = (img_blend - img_blend.min()) / (img_blend.max() - img_blend.min())\n",
    "    \n",
    "    return img_blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc9e71",
   "metadata": {
    "id": "a7bc9e71",
    "outputId": "d2eb0b58-6e57-4da6-89f5-2e09572d6915"
   },
   "outputs": [],
   "source": [
    "img, target = testset[0]\n",
    "img_show = img.permute(1,2,0).numpy()\n",
    "\n",
    "# target\n",
    "class_name = list(testset.class_to_idx.keys())\n",
    "target_name = class_name[target]\n",
    "\n",
    "# with pretrained weights\n",
    "add_attn_vis_hook(model_wo_pretrained)\n",
    "output = model_wo_pretrained(img.unsqueeze(0).to(cfg.device))\n",
    "pred_wo_pretrained = output.argmax(dim=1)[0].item()\n",
    "prob_wo_pretrained = nn.functional.softmax(output, dim=1)[0][pred_wo_pretrained]\n",
    "\n",
    "img_blend_wo_pretrained = blend_attention_with_image(image=img_show, activation=activation)\n",
    "\n",
    "# with pretrained weights\n",
    "add_attn_vis_hook(model_w_pretrained)\n",
    "output = model_w_pretrained(img.unsqueeze(0).to(cfg.device))\n",
    "pred_w_pretrained = output.argmax(dim=1)[0].item()\n",
    "prob_w_pretrained = nn.functional.softmax(output, dim=1)[0][pred_w_pretrained]\n",
    "\n",
    "img_blend_w_pretrained = blend_attention_with_image(image=img_show, activation=activation)\n",
    "\n",
    "# figure\n",
    "fig, ax = plt.subplots(1,3,figsize=(12,5))\n",
    "\n",
    "ax[0].imshow(img_show)\n",
    "ax[1].imshow(img_blend_wo_pretrained)\n",
    "ax[2].imshow(img_blend_w_pretrained)\n",
    "\n",
    "# set title\n",
    "ax[0].set_title(f'Image\\nTarget: {target_name}')\n",
    "ax[1].set_title(f'ViT-B/8\\nwithout pretrained weights\\nPred: {class_name[pred_wo_pretrained]} ({prob_wo_pretrained:.2%})')\n",
    "ax[2].set_title(f'ViT-B/8\\nwith pretrained weights\\nPred: {class_name[pred_w_pretrained]} ({prob_w_pretrained:.2%})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953f8989-8126-4ba4-bf3d-145cc04cc0c1",
   "metadata": {
    "id": "c2452279"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
